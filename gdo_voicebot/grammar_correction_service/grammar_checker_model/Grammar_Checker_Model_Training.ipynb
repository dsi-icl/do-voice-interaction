{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Grammar_Checker_Model_Training.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GDO-Galileo/do-voice-interaction/blob/error_correction/gdo_voicebot/grammar_correction_service/grammar_checker_model/Grammar_Checker_Model_Training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zqpV7rTqetFA"
      },
      "source": [
        "# **Training the BERT uncased model into a grammatical error checker**\n",
        "\n",
        "This notebook creates a custom language model consisting of an extra layer added to the pretrained BERT base model (uncased) and training it to predict if an input sentence is grammatically correct or incorrect. This is achieved in two main stages:\n",
        "\n",
        "1.   **Pre-training:** the custom model is pre-trained using the English data of the [Lang-8 Corpus of Learner English](https://sites.google.com/site/naistlang8corpora/), a dataset compiled from a language exchange social network service. \n",
        "2.   **Training:** the custom model is trained using in-domain data from the [Corpus of Linguistic Acceptability (CoLA)](https://nyu-mll.github.io/CoLA/) consisting of sentences from 23 linguistic publications.\n",
        "\n",
        "For **validation** we use a sample of 10% of CoLA's in-domain dataset, and for **testing** we use CoLA's out-of-domain dev set.\n",
        "\n",
        "## **Pre-Processing**\n",
        "Both datasets are already pre-processed when inputed to the model using methods described our [Pre-Processing_Datasets](https://colab.research.google.com/drive/1is-XvliKcN4XXTK167TEeutVOvYB6Nqn?usp=sharing) file. The actual data is not modified during preprocessing, though punctuation apart from apostrophes is removed and sentences are made lowercase (as is the case for the GDO's speech-to-text system).\n",
        "\n",
        "For the **CoLA** dataset, sentences labeled as 'incorrect' are oversampled to balance the distribution of the dataset. For the **Lang-8** dataset, a random sample of around 20,000 sentences is used (out of a possible 1,000,000 to reduce training time). Lang-8 is undersampled if the random 20,000 samples are of an uneven distribution. The actual pre-processed datasets we used for our trained model can be found [here](https://imperialcollegelondon.box.com/s/0s602z27cgzq6fxk5lhb8446li8030ta).\n",
        "\n",
        "The **input dataset** for the model is in the format:\n",
        "```\n",
        "  Column     Description\n",
        " ------------------------------------------------------------------------------------------\n",
        "    1\t    the acceptability judgment label (0 = unacceptable, 1 = acceptable).\n",
        "    2 \t   the lowercase sentence with no puntuation apart from apostrophes.\n",
        "```\n",
        "For example, a sample from 'cola-train.tsv' reads:\n",
        "```\n",
        "  1     john and the man went to the store\n",
        "  0     i loved intensely the policeman with all my heart\n",
        "  0     i'm sure we got any tickets\n",
        "  1     the umpire called the game off\n",
        "```\n",
        "\n",
        "## **Hyperparameter Tuning**\n",
        "\n",
        "The hyperparameters we chose to modify were the **learning rate** and **number of epochs**. Hyperparameter tuning was done using 10-fold cross-validation twice: first training with learning rates from 0.0001 to 0.1 and then between 0.0223 and 0.0334 since these were the best performing learning rates in general. We plotted metrics using the output of the file, as can be seen in our [Plot_Metrics](https://colab.research.google.com/drive/1Xa6VR26_FpDcx7boygx69mJe4MrAlifM?usp=sharing) file, and used these to decide on the number of epochs for each final model (after pre-training and after training).\n",
        "\n",
        "## **Using the Model**\n",
        "\n",
        "As shown in the validation and testing stages, predictions can be put through a sigmoid function and rounded to produce a predicted label (0 = unacceptable/incorrect, 1 = acceptable/correct). There is an example of this [here](https://colab.research.google.com/drive/1AjT56COhhcABOLmSQFau_djdhWEPRwwb?usp=sharing)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oUTuU8LwLzoP",
        "outputId": "e17aeb81-65e5-4d02-ffec-0aff46a41afe"
      },
      "source": [
        "##################################\n",
        "#   Mount Drive to Save Models   #\n",
        "##################################\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Folder path to where models will be saved\n",
        "# Note: folders must already exist to save them there\n",
        "folder_path = './drive/My Drive/Galileo/models/'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 178
        },
        "id": "CW-lyG9RfoH3",
        "outputId": "328fce23-0615-4784-e8be-e931f5de93cf"
      },
      "source": [
        "###################################\n",
        "#         Upload datasets         #\n",
        "###################################\n",
        "\n",
        "# Upload all of:\n",
        "#   lang-8-train.tsv\n",
        "#   cola-train.tsv\n",
        "#   cola-validate.tsv\n",
        "#   cola-test.tsv\n",
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-68f0edb6-9075-4cb9-ad9d-e3792b95c593\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-68f0edb6-9075-4cb9-ad9d-e3792b95c593\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving cola-test.tsv to cola-test.tsv\n",
            "Saving cola-train.tsv to cola-train.tsv\n",
            "Saving cola-validate.tsv to cola-validate.tsv\n",
            "Saving lang-8-train.tsv to lang-8-train.tsv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8G-EGqR9Obeu"
      },
      "source": [
        "###################################\n",
        "#             Imports             #\n",
        "###################################\n",
        "\n",
        "!pip install transformers\n",
        "\n",
        "import io \n",
        "import os\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from transformers import BertTokenizer, BertModel\n",
        "import torch.nn as nn\n",
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pDIQ523xPPI1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "491d2a86-5ede-4b9c-9499-9b4cb9f3527e"
      },
      "source": [
        "###################################\n",
        "#          Set Constants          #\n",
        "###################################\n",
        "\n",
        "## Changeable Constants ##\n",
        "# Learning rate of the optimiser\n",
        "LEARNING_RATE = 0.0223\n",
        "\n",
        "# Batch size (for Google Colab this should be 32 so as to not run out of memory)\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "## Unchangeable Constants ##\n",
        "# Sentence acceptability labels\n",
        "CORRECT = 1\n",
        "INCORRECT = 0\n",
        "\n",
        "## Find and set the current device ##\n",
        "# Use graphics card if available, otherwise use CPU\n",
        "if torch.cuda.is_available():\n",
        "  device = torch.device(\"cuda\")\n",
        "else:\n",
        "  device = torch.device(\"cpu\")\n",
        "\n",
        "n_gpu = torch.cuda.device_count()\n",
        "torch.cuda.get_device_name(0)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Tesla K80'"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K5xqvYxtTGIr"
      },
      "source": [
        "## **Model Evaluation Functions**\n",
        "\n",
        "For evaluating our model, we first compute the confusion matrix:\n",
        "\n",
        "```\n",
        "            Predicted 0 | Predicted 1\n",
        "  Actual 0 | [[  TN     ,     FP  ],\n",
        "  Actual 1 |  [  FN     ,     FN  ]]\n",
        "\n",
        "  Where:  TN = [0][0] True Negative\n",
        "          FN = [1][0] False Negative\n",
        "          FP = [0][1] False Positive\n",
        "          TP = [1][1] True Positive\n",
        "```\n",
        "\n",
        "This is done by taking the predictions from the model and converting them to either a '1' or '0' using `np.round(sigmoid(y_pred))`. Sigmoid is used due to our optimiser being BCEWithLogitsLoss (which includes a sigmoid activation function on top of the Binary Cross-Entropy loss function).\n",
        "\n",
        "The metrics we calculate using the confusion matrix are:\n",
        "\n",
        "*   Recall\n",
        "*   Precision\n",
        "*   F1 Measure\n",
        "*   Accuracy\n",
        "\n",
        "Functions for all of which can be found below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lx814jpUWv7Y"
      },
      "source": [
        "###################################\n",
        "#      Evaluation Functions       #\n",
        "###################################\n",
        "\n",
        "def sigmoid(x):\n",
        "    return 1/(1 + np.exp(-x))\n",
        "\n",
        "# Computes a confusion matrix of the format above based on y_pred (predictions)\n",
        "#   and y_test (gold)\n",
        "# Note: y_pred and y_test should be numpy arrays\n",
        "def confusion_matrix(y_pred, y_test):\n",
        "  # For each prediction, convert to a tag of 0 or 1\n",
        "  y_pred_tag = np.round(sigmoid(y_pred)).astype('int32')\n",
        "  y_test = y_test.astype('int32')\n",
        "\n",
        "  # Since this is binary classification, the confusion matrix is (2, 2)\n",
        "  confusion = np.zeros((2, 2), dtype=np.int)\n",
        "\n",
        "  # Count each class for each prediction\n",
        "  for i in range(len(y_pred_tag)):\n",
        "    confusion[y_test[i], y_pred_tag[i]] += 1\n",
        "\n",
        "  return confusion\n",
        "\n",
        "# Calculates precision from a confusion matrix of the format above\n",
        "# Note: class_label should be CORRECT or INCORRECT\n",
        "def precision(confusion, class_label):\n",
        "  true_pos = confusion[1][class_label]\n",
        "  false_pos = confusion[0][class_label]\n",
        "\n",
        "  return true_pos / (true_pos + false_pos)\n",
        "\n",
        "# Calculates macro average precision from a confusion matrix \n",
        "#   of the format above\n",
        "def marco_avg_precision(confusion):\n",
        "  correct_precision = precision(confusion, CORRECT)\n",
        "  incorrect_precision = precision(confusion, INCORRECT)\n",
        "\n",
        "  return (correct_precision + incorrect_precision) / 2\n",
        "\n",
        "# Calculates recall from s a confusion matrix of the format above\n",
        "# Note: class_label should be CORRECT or INCORRECT\n",
        "def recall(confusion, class_label):\n",
        "  true_pos = confusion[class_label][1]\n",
        "  false_neg = confusion[class_label][0]\n",
        "\n",
        "  return true_pos / (true_pos + false_neg)\n",
        "\n",
        "# Calculates macro average recall from a confusion matrix of the format above\n",
        "def macro_avg_recall(confusion):\n",
        "  correct_recall = recall(confusion, CORRECT)\n",
        "  incorrect_recall = recall(confusion, INCORRECT)\n",
        "\n",
        "  return (correct_recall + incorrect_recall) / 2\n",
        "\n",
        "# Calculates f1 measure from a confusion matrix of the format above\n",
        "# Note: class_label should be CORRECT or INCORRECT\n",
        "def f_one_measure(confusion, class_label):\n",
        "  total_precision = precision(confusion, class_label)\n",
        "  total_recall = recall(confusion, class_label)\n",
        "\n",
        "  return (2 * total_precision * total_recall) / (total_precision + total_recall)\n",
        "\n",
        "# Calculates the average f1 measure from a confusion matrix of the format above\n",
        "def avg_f_one_measure(confusion):\n",
        "  correct_f_one = f_one_measure(confusion, CORRECT)\n",
        "  incorrect_f_one = f_one_measure(confusion, INCORRECT)\n",
        "\n",
        "  return (correct_f_one + incorrect_f_one) / 2\n",
        "\n",
        "# Calculates the accuracy from a confusion matrix of the format above\n",
        "def accuracy(confusion):\n",
        "  true_pos = confusion[1][1]\n",
        "  true_neg = confusion[0][0]\n",
        "\n",
        "  total = sum([np.sum(row) for row in confusion])\n",
        "\n",
        "  return (true_pos + true_neg) / total"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UGD2ncwqXuH4"
      },
      "source": [
        "## **Custom BERT Model Class**\n",
        "\n",
        "We create our own custom grammar checker model by adding an extra layer to the pre-trained BERT base model (uncased due to the GDO's speech-to-text system being lowercase). We previously attempted using BertForSequenceClassification as a base, but this way we are able to output only one value as a prediction.\n",
        "\n",
        "We also modified the `state_dict()` and `load_state_dict()` functions to enable us to only save and load the weights of the final layer â€” vastly reducing the amount of storage a model takes up."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SBakTrem5ey-"
      },
      "source": [
        "##################################\n",
        "#          Model Class           #\n",
        "##################################\n",
        "# with reference to https://stackoverflow.com/questions/64156202/add-dense-layer-on-top-of-huggingface-bert-model\n",
        "# and documentation at https://huggingface.co/transformers/model_doc/bert.html#bertmodel\n",
        "\n",
        "class CustomBERTModel(torch.nn.Module):\n",
        "  def __init__(self):\n",
        "    super(CustomBERTModel, self).__init__()\n",
        "    self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
        "    ## New Layer\n",
        "    self.linear = torch.nn.Linear(768, 1)\n",
        "\n",
        "  # A forward pass through both the BERT model and linear layer\n",
        "  def forward(self, input_ids):\n",
        "    outputs = self.bert(input_ids, token_type_ids=None)\n",
        "\n",
        "    # Gets the ouput of the last hidden layer of the BERT model\n",
        "    last_hidden_states = outputs.last_hidden_state\n",
        "    linear_output = self.linear(last_hidden_states[:,0,:])\n",
        "\n",
        "    return linear_output\n",
        "\n",
        "  # Modified state_dict to only save linear layer weights and bias\n",
        "  def state_dict(self):\n",
        "    return self.linear.state_dict()\n",
        "\n",
        "  # Modified load_state_dict to only load linear layer weights and bias\n",
        "  def load_state_dict(self, state_dict):\n",
        "    self.linear.load_state_dict(state_dict)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YGvEwhrhXVpf"
      },
      "source": [
        "## **Loading Models**\n",
        "\n",
        "Before training, we create our model and load it to the currect device (as defined in the 'Constants' section above).\n",
        "\n",
        "If training is continuing, you may upload the model above and load it with the second cell of code."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "10qGBbKsTWQw"
      },
      "source": [
        "# Load new custom model to the GPU\n",
        "model = CustomBERTModel()\n",
        "model.cuda()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A2WZft2nYMDE"
      },
      "source": [
        "# Load a model after uploading with the name:\n",
        "#   bert-base-uncased-GDO-trained.pth\n",
        "model = CustomBERTModel()\n",
        "model.load_state_dict(torch.load('bert-base-uncased-GDO-trained.pth', map_location=device))\n",
        "model.cuda()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LVnzxFKFaWln"
      },
      "source": [
        "## **Optimiser and Loss Function**\n",
        "\n",
        "For the optimisation, we use pytorch's Stochastic Gradient Descent optimiser. As the imported BERT model is already pre-trained, faster learning optimisers such as Adam have proved to overfit the data in just a couple of epochs.\n",
        "\n",
        "We use the `BCEWithLogitsLoss` function (Binary Cross-Entropy with a sigmoid activation function) as our loss function, as this is widely used for binary classification."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-8En5uyog7zs"
      },
      "source": [
        "###################################\n",
        "#       Optimiser and Loss        #\n",
        "###################################\n",
        "\n",
        "optimiser = torch.optim.SGD(model.parameters(), lr=LEARNING_RATE)\n",
        "criterion = nn.BCEWithLogitsLoss()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q443O9ZhgAqA"
      },
      "source": [
        "## **Data Tokenization**\n",
        "\n",
        "For tokenizing we use the standard BERT base uncased tokenizer with the `do_lower_case` flag set to true (since this is the case for the GDO speech-to-text system)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DI7a8VVvgzcR"
      },
      "source": [
        "##################################\n",
        "#           Tokenizer            #\n",
        "##################################\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', \n",
        "                                          do_lower_case = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zGATnSADdsfp"
      },
      "source": [
        "## **Pre-Training**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sUhh1nKGvMsj"
      },
      "source": [
        "### **Preparing Datasets**\n",
        "For **pre-training** we use the **Lang-8 train** dataset (in the uploaded `lang-8-train.tsv` file). We tokenize the input sentences using the tokenizer, map the resulting tokens to IDs and pad the outputs to the maximum length in the IDs sequence. When loading the training dataset into a DataLoader, we use a `RandomSampler` so the indexes will be shuffled for each epoch. \n",
        "\n",
        "For **validation** we use a 10% sample of the **CoLA in-domain train** dataset (in the uploaded `cola-validate.tsv` file) and process the data in the same way as for pre-training. When loading the validation dataset into a DataLoader, we use a `SequentialSampler` so the indexes will be in the same order for each epoch. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XHyfVWWu6m7p"
      },
      "source": [
        "###################################\n",
        "#          Training Set           #\n",
        "###################################\n",
        "\n",
        "df = pd.read_csv(io.BytesIO(uploaded['lang-8-train.tsv']), delimiter='\\t', header=None,\n",
        "  names=['label', 'sentence'])\n",
        "\n",
        "# Create sentence and label lists\n",
        "sentences = df.sentence.values\n",
        "labels = df.label.values\n",
        "tokenized_texts = [tokenizer.tokenize(str(sent)) for sent in sentences]\n",
        "\n",
        "# Padding sentences to the maximum length sentence\n",
        "padded_sequence = \\\n",
        "  [tokenizer.convert_tokens_to_ids(txt) for txt in tokenized_texts]\n",
        "max_len = max([len(txt) for txt in padded_sequence])\n",
        "\n",
        "# Pad our input tokens\n",
        "input_ids = pad_sequences(padded_sequence, maxlen=max_len, dtype=\"long\", \n",
        "                          truncating=\"post\", padding=\"post\")\n",
        "\n",
        "# Create input and label matrices\n",
        "train_inputs = torch.tensor(input_ids)\n",
        "train_labels = torch.tensor(labels)\n",
        "\n",
        "# Create iterator from formatted training data\n",
        "train_data = TensorDataset(train_inputs, train_labels)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = \\\n",
        "  DataLoader(train_data, sampler=train_sampler, batch_size=BATCH_SIZE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_wE_t_eN59Wq"
      },
      "source": [
        "###################################\n",
        "#         Validation Set          #\n",
        "###################################\n",
        "\n",
        "df = pd.read_csv(io.BytesIO(uploaded['cola-validate.tsv']), delimiter='\\t', header=None,\n",
        "  names=['label', 'sentence'])\n",
        "\n",
        "# Create sentence and label lists\n",
        "sentences = df.sentence.values\n",
        "labels = df.label.values\n",
        "tokenized_texts = [tokenizer.tokenize(str(sent)) for sent in sentences]\n",
        "\n",
        "# Padding sentences to the maximum length sentence\n",
        "padded_sequence = [tokenizer.convert_tokens_to_ids(txt) for txt in tokenized_texts]\n",
        "max_len = max([len(txt) for txt in padded_sequence])\n",
        "\n",
        "# Pad our input tokens\n",
        "input_ids = pad_sequences(padded_sequence, maxlen=max_len, dtype=\"long\", \n",
        "                          truncating=\"post\", padding=\"post\")\n",
        "\n",
        "# Create input and label matrices\n",
        "validation_inputs = torch.tensor(input_ids)\n",
        "validation_labels = torch.tensor(labels)\n",
        "\n",
        "# Create iterator from formatted training data\n",
        "validation_data = \\\n",
        "  TensorDataset(validation_inputs, validation_labels)\n",
        "validation_sampler = SequentialSampler(validation_data)\n",
        "validation_dataloader = \\\n",
        "  DataLoader(validation_data, sampler=validation_sampler, batch_size=BATCH_SIZE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B6psR4oahiPM"
      },
      "source": [
        "### **Pre-Training Loop**\n",
        "\n",
        "Similarly to the training loop, the pre-training loop has gone through several versions before this one.\n",
        "\n",
        "*   In our first version, we set the **number of epochs to be 3** every time, only modifying the learning rate between models.\n",
        "*   In our second version, we used the **validation accuracy** to decide when to finish training. We would store the best accuracy, and if it decreased we would revert to the previous iteration's model (the one with the best accuracy) and continue to training. Since this method only took accuracy into account, the performance of the models wasn't very reliable.\n",
        "*   In the final version, we train with **up to 25 epochs**. After this, we use our [Plot_Metrics](https://colab.research.google.com/drive/1Xa6VR26_FpDcx7boygx69mJe4MrAlifM#scrollTo=VEDZ3rjp53ms) file to determine the best models based on our output metrics, load these and continue to training.\n",
        "\n",
        "We also previously included a way of plotting the loss per batch (based on the version used [here](https://github.com/sunilchomal/GECwBERT/blob/master/BERT_GED_Model.ipynb)), but with the final version loading already trained models this wasn't possible to include."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OO0gU8W0hJ7z"
      },
      "source": [
        "##################################\n",
        "#       Pre-Training Loop        #\n",
        "##################################\n",
        "\n",
        "# Store the current epoch number\n",
        "epochs = 0\n",
        "\n",
        "# Iterate for up to 25 epochs\n",
        "while epochs < 25:\n",
        "  epochs += 1\n",
        "  \n",
        "  # Tracking variables (nb = Naive Bayes, tr=Tracking)\n",
        "  tr_loss = 0\n",
        "  nb_tr_steps = 0\n",
        "\n",
        "  print(\"Processing Epoch Number: {}\".format(epochs))\n",
        "  \n",
        "\n",
        "  ## Training ##\n",
        "\n",
        "  # Set the model to training mode\n",
        "  model.train()\n",
        "  \n",
        "  # Train the data for one epoch\n",
        "  for step, batch in enumerate(train_dataloader):\n",
        "    # Add batch to GPU\n",
        "    batch = tuple(t.to(device) for t in batch)\n",
        "    \n",
        "    # Unpack the inputs from the dataloader\n",
        "    b_input_ids, b_labels = batch\n",
        "\n",
        "    # Clear out the gradients\n",
        "    optimiser.zero_grad()\n",
        "\n",
        "    # Forward pass\n",
        "    # Shape of outputs -> (batch_size, num_features)\n",
        "    #                     (so in this case 'torch.Size([32, 1])')\n",
        "    outputs = model(b_input_ids)\n",
        "\n",
        "    # Make b_labels the same shape as outputs and convert to float\n",
        "    #     (i.e. from 'torch.Size([32])' to 'torch.Size([32, 1])')\n",
        "    b_labels = b_labels.unsqueeze(1)\n",
        "    b_labels = b_labels.float()\n",
        "\n",
        "    # Calculate loss\n",
        "    loss = criterion(outputs, b_labels)\n",
        "\n",
        "    # Backward pass\n",
        "    loss.backward()\n",
        "\n",
        "    # Update parameters and take a step\n",
        "    optimiser.step()\n",
        "\n",
        "    # Update tracking variables\n",
        "    tr_loss += loss.item()\n",
        "    nb_tr_steps += 1\n",
        "\n",
        "  # Total loss for this epoch\n",
        "  print(\" Train loss: {}\".format(tr_loss/nb_tr_steps))\n",
        "    \n",
        "\n",
        "  ## Validation ##\n",
        "\n",
        "  # Put model in evaluation mode\n",
        "  model.eval()\n",
        "\n",
        "  # Initialise confusion matrix\n",
        "  confusion = np.zeros((2, 2), dtype=np.int)\n",
        "\n",
        "  # Evaluate data for one epoch\n",
        "  for batch in validation_dataloader:\n",
        "    # Add batch to GPU\n",
        "    batch = tuple(t.to(device) for t in batch)\n",
        "\n",
        "    # Unpack the inputs from our dataloader\n",
        "    b_input_ids, b_labels = batch\n",
        "    \n",
        "    # Don't compute or store gradients\n",
        "    with torch.no_grad():\n",
        "      # Forward pass, calculate logit predictions (predicted values)\n",
        "      logits = model(b_input_ids)\n",
        "      \n",
        "    # Move logits and labels to CPU\n",
        "    logits = logits.detach().cpu().numpy()\n",
        "    label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "    # Calculate confusion matrix\n",
        "    confusion += confusion_matrix(logits, label_ids)\n",
        "\n",
        "  # Print calculated metrics for this epoch\n",
        "  print(\" Confustion Matrix:\\n \" + str(confusion))\n",
        "  print(\"\")\n",
        "  print(\" Validation Accuracy: {}\".format(accuracy(confusion)))\n",
        "  print(\"\")\n",
        "  print(\" Validation Correct Recall: {}\".format(recall(confusion, CORRECT)))\n",
        "  print(\" Validation Incorrect Recall: {}\".format(recall(confusion, INCORRECT)))\n",
        "  print(\" Validation Total Recall: {}\".format(macro_avg_recall(confusion)))\n",
        "  print(\"\")\n",
        "  print(\" Validation Correct Precision: {}\".format(precision(confusion, CORRECT)))\n",
        "  print(\" Validation Incorrect Precision: {}\".format(precision(confusion, INCORRECT)))\n",
        "  print(\" Validation Total Precision: {}\".format(marco_avg_precision(confusion)))\n",
        "  print(\"\")\n",
        "  print(\" Validation Correct F1: {}\".format(f_one_measure(confusion, CORRECT)))\n",
        "  print(\" Validation Incorrect F1: {}\".format(f_one_measure(confusion, INCORRECT)))\n",
        "  print(\" Validation Total F1: {}\".format(avg_f_one_measure(confusion)))\n",
        "  print(\"\")\n",
        "\n",
        "\n",
        "  ## Saving the Model for this Epoch ##\n",
        "\n",
        "  torch.save(model.state_dict(), 'bert-base-uncased-GDO-trained.pth')\n",
        "  os.system('cp bert-base-uncased-GDO-trained.pth \"{}/bert-base-uncased-GDO-{}-lang-8.pth\"'.format(folder_path, str(epochs)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w8MwhiX3jaAw"
      },
      "source": [
        "## **Training**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y4W5bzK0vING"
      },
      "source": [
        "### **Preparing Datasets**\n",
        "\n",
        "For **training** we use a 90% sample of the **CoLA in-domain train** dataset (in the uploaded `cola-train.tsv` file). As for pre-training, we tokenize the input sentences using the tokenizer, map the resulting tokens to IDs and pad the outputs to the maximum length in the IDs sequence. When loading the training dataset into a DataLoader, we also use a `RandomSampler` so the indexes will be shuffled for each epoch. \n",
        "\n",
        "For **validation** we use the same set as for pre-training: a 10% sample of the **CoLA in-domain train** dataset (in the uploaded `cola-validate.tsv` file). The same DataLoader as for pre-training is used for training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eym3uUMNQlgF"
      },
      "source": [
        "# Training Set\n",
        "df = pd.read_csv(io.BytesIO(uploaded['cola-train.tsv']), delimiter='\\t', \n",
        "                 header=None, names=['label', 'sentence'])\n",
        "\n",
        "# Create sentence and label lists\n",
        "sentences = df.sentence.values\n",
        "labels = df.label.values\n",
        "\n",
        "tokenized_texts = [tokenizer.tokenize(str(sent)) for sent in sentences]\n",
        "\n",
        "# Padding sentences to the maximum length sentence\n",
        "padded_sequence = \\\n",
        "  [tokenizer.convert_tokens_to_ids(txt) for txt in tokenized_texts]\n",
        "max_len = max([len(txt) for txt in padded_sequence])\n",
        "\n",
        "# Pad our input tokens\n",
        "input_ids = pad_sequences(padded_sequence, maxlen=max_len, dtype=\"long\", \n",
        "                          truncating=\"post\", padding=\"post\")\n",
        "\n",
        "# Create input and label matrices\n",
        "train_inputs = torch.tensor(input_ids)\n",
        "train_labels = torch.tensor(labels)\n",
        "\n",
        "# Create iterator from formatted training data\n",
        "train_data = TensorDataset(train_inputs, train_labels)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = \\\n",
        "  DataLoader(train_data, sampler=train_sampler, batch_size=BATCH_SIZE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "86GNCWEljvhr"
      },
      "source": [
        "### **Training Loop**\n",
        "\n",
        "Similarly to the pre-training loop, the training loop has gone through several versions before this one.\n",
        "\n",
        "*   In our first version, we set the **number of epochs to be 5** every time, only modifying the learning rate between models.\n",
        "*   In our second version, like pre-training, we used the **validation accuracy** to decide when to finish training. We used the same method as for pre-training.\n",
        "*   In our third version, we used a **set number of epochs for pre-training** and the **validation accuracy for training**, since our models would generally overfit to pre-training data.\n",
        "*   In the final version, we take a model saved from pre-training, and train for a further **up to 50 epochs**. After this, we again use our [Plot_Metrics](https://colab.research.google.com/drive/1Xa6VR26_FpDcx7boygx69mJe4MrAlifM?usp=sharing) file to determine the final model based on the output metrics."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-epNNEg6VK28"
      },
      "source": [
        "##################################\n",
        "#         Training Loop          #\n",
        "##################################\n",
        "\n",
        "# Store the current epoch number\n",
        "epochs = 0\n",
        "\n",
        "# Iterate for up to 50 epochs\n",
        "while epochs < 50:\n",
        "  epochs += 1\n",
        "  \n",
        "  # Tracking variables (nb = Naive Bayes, tr=Tracking)\n",
        "  tr_loss = 0\n",
        "  nb_tr_steps = 0\n",
        "\n",
        "  print(\"Processing Epoch Number: {}\".format(epochs))\n",
        "  \n",
        "\n",
        "  ## Training ##\n",
        "\n",
        "  # Set the model to training mode\n",
        "  model.train()\n",
        "  \n",
        "  # Train the data for one epoch\n",
        "  for step, batch in enumerate(train_dataloader):\n",
        "    # Add batch to GPU\n",
        "    batch = tuple(t.to(device) for t in batch)\n",
        "    \n",
        "    # Unpack the inputs from the dataloader\n",
        "    b_input_ids, b_labels = batch\n",
        "\n",
        "    # Clear out the gradients\n",
        "    optimiser.zero_grad()\n",
        "\n",
        "    # Forward pass\n",
        "    # Shape of outputs -> (batch_size, num_features)\n",
        "    #                     (so in this case 'torch.Size([32, 1])')\n",
        "    outputs = model(b_input_ids)\n",
        "\n",
        "    # Make b_labels the same shape as outputs and convert to float\n",
        "    #     (i.e. from 'torch.Size([32])' to 'torch.Size([32, 1])')\n",
        "    b_labels = b_labels.unsqueeze(1)\n",
        "    b_labels = b_labels.float()\n",
        "\n",
        "    # Calculate loss\n",
        "    loss = criterion(outputs, b_labels)\n",
        "\n",
        "    # Backward pass\n",
        "    loss.backward()\n",
        "\n",
        "    # Update parameters and take a step\n",
        "    optimiser.step()\n",
        "\n",
        "    # Update tracking variables\n",
        "    tr_loss += loss.item()\n",
        "    nb_tr_steps += 1\n",
        "\n",
        "  # Total loss for this epoch\n",
        "  print(\" Train loss: {}\".format(tr_loss/nb_tr_steps))\n",
        "    \n",
        "\n",
        "  ## Validation ##\n",
        "\n",
        "  # Put model in evaluation mode\n",
        "  model.eval()\n",
        "\n",
        "  # Initialise confusion matrix\n",
        "  confusion = np.zeros((2, 2), dtype=np.int)\n",
        "\n",
        "  # Evaluate data for one epoch\n",
        "  for batch in validation_dataloader:\n",
        "    # Add batch to GPU\n",
        "    batch = tuple(t.to(device) for t in batch)\n",
        "\n",
        "    # Unpack the inputs from our dataloader\n",
        "    b_input_ids, b_labels = batch\n",
        "    \n",
        "    # Don't compute or store gradients\n",
        "    with torch.no_grad():\n",
        "      # Forward pass, calculate logit predictions (predicted values)\n",
        "      logits = model(b_input_ids)\n",
        "      \n",
        "    # Move logits and labels to CPU\n",
        "    logits = logits.detach().cpu().numpy()\n",
        "    label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "    # Calculate confusion matrix\n",
        "    confusion += confusion_matrix(logits, label_ids)\n",
        "\n",
        "  # Print calculated metrics for this epoch\n",
        "  print(\" Confusion Matrix:\\n \" + str(confusion))\n",
        "  print(\"\")\n",
        "  print(\" Validation Accuracy: {}\".format(accuracy(confusion)))\n",
        "  print(\"\")\n",
        "  print(\" Validation Correct Recall: {}\".format(recall(confusion, CORRECT)))\n",
        "  print(\" Validation Incorrect Recall: {}\".format(recall(confusion, INCORRECT)))\n",
        "  print(\" Validation Total Recall: {}\".format(macro_avg_recall(confusion)))\n",
        "  print(\"\")\n",
        "  print(\" Validation Correct Precision: {}\".format(precision(confusion, CORRECT)))\n",
        "  print(\" Validation Incorrect Precision: {}\".format(precision(confusion, INCORRECT)))\n",
        "  print(\" Validation Total Precision: {}\".format(marco_avg_precision(confusion)))\n",
        "  print(\"\")\n",
        "  print(\" Validation Correct F1: {}\".format(f_one_measure(confusion, CORRECT)))\n",
        "  print(\" Validation Incorrect F1: {}\".format(f_one_measure(confusion, INCORRECT)))\n",
        "  print(\" Validation Total F1: {}\".format(avg_f_one_measure(confusion)))\n",
        "  print(\"\")\n",
        "\n",
        "\n",
        "  ## Saving the Model for this Epoch ##\n",
        "  \n",
        "  torch.save(model.state_dict(), 'bert-base-uncased-GDO-trained.pth')\n",
        "  os.system('cp bert-base-uncased-GDO-trained.pth \"{}/bert-base-uncased-GDO-{}-lang-8-cola.pth\"'.format(folder_path, str(epochs)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CaYna0Wwl1bv"
      },
      "source": [
        "## **Testing**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TuFVwbdqvDjY"
      },
      "source": [
        "### **Preparing Datasets**\n",
        "\n",
        "For **testing** we use the **CoLA out-of-domain dev** dataset (in the uploaded `cola-test.tsv` file). As before, we tokenize the input sentences using the tokenizer, map the resulting tokens to IDs and pad the outputs to the maximum length in the IDs sequence. When loading the training dataset into a DataLoader, we use a `SequentialSampler` so the indexes will be in the same order for each epoch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6BvwuYVxZ4Qw"
      },
      "source": [
        "df = pd.read_csv(io.BytesIO(uploaded['cola-test.tsv']), delimiter='\\t', \n",
        "                 header=None, names=['label', 'sentence'])\n",
        "\n",
        "# Create sentence and label lists\n",
        "sentences = df.sentence.values\n",
        "labels = df.label.values\n",
        "\n",
        "tokenized_texts = [tokenizer.tokenize(str(sent)) for sent in sentences]\n",
        "\n",
        "# Padding sentences to the maximum length sentence\n",
        "padded_sequence = [tokenizer.convert_tokens_to_ids(txt) for txt in tokenized_texts]\n",
        "max_len = max([len(txt) for txt in padded_sequence])\n",
        "\n",
        "# Pad our input tokens\n",
        "input_ids = pad_sequences(padded_sequence, maxlen=max_len, dtype=\"long\", \n",
        "                          truncating=\"post\", padding=\"post\")\n",
        "\n",
        "# Create input and label matrices\n",
        "prediction_inputs = torch.tensor(input_ids)\n",
        "prediction_labels = torch.tensor(labels)\n",
        "\n",
        "# Create iterator from formatted training data\n",
        "prediction_data = \\\n",
        "  TensorDataset(prediction_inputs, prediction_labels)\n",
        "prediction_sampler = SequentialSampler(prediction_data)\n",
        "prediction_dataloader = \\\n",
        "  DataLoader(prediction_data, sampler=prediction_sampler, batch_size=BATCH_SIZE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ds3u26XTspAt"
      },
      "source": [
        "### **Test Epoch**\n",
        "\n",
        "This epoch is very similar to a validation epoch. However, rather than go through each batch seperately, each prediction for each batch is added to a list to more easily calculate metrics."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gPf51Y7aaTvV"
      },
      "source": [
        "#################################\n",
        "#          Test Epoch           #\n",
        "#################################\n",
        "\n",
        "# Put model in evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Tracking variables\n",
        "predictions, true_labels = [], []\n",
        "\n",
        "# Predict\n",
        "for batch in prediction_dataloader:\n",
        "  # Add batch to GPU\n",
        "  batch = tuple(t.to(device) for t in batch)\n",
        "\n",
        "  # Unpack the inputs from our dataloader\n",
        "  b_input_ids, b_labels = batch\n",
        "\n",
        "  # Don't compute or store gradients\n",
        "  with torch.no_grad():\n",
        "    # Forward pass\n",
        "    logits = model(b_input_ids)\n",
        "  \n",
        "  # Move logits and labels to CPU\n",
        "  logits = logits.detach().cpu().numpy()\n",
        "  label_ids = b_labels.to(\"cpu\").numpy()\n",
        "  \n",
        "  # Store predictions and true labels\n",
        "  predictions.append(logits)\n",
        "  true_labels.append(label_ids)\n",
        "\n",
        "# Flatten predictions and true_labels into one list each\n",
        "flat_predictions = [item for sublist in predictions for item in sublist]\n",
        "flat_predictions = np.round(sigmoid(np.array(flat_predictions)))\n",
        "flat_true_labels = [item for sublist in true_labels for item in sublist]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8_gzZ3BCue6W"
      },
      "source": [
        "### **Test Evaluation**\n",
        "\n",
        "We evaluate the test predictions both using the same metrics as before (**recall**, **precision**, **F1 measure** and **accuracy**) and matthew's correlation coefficient, based on the method [here](https://github.com/sunilchomal/GECwBERT/blob/master/BERT_GED_Model.ipynb)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mhiwma4atOIB"
      },
      "source": [
        "##################################\n",
        "#         Print Metrics          #\n",
        "##################################\n",
        "\n",
        "confusion = confusion_matrix(flat_predictions, flat_true_labels)\n",
        "\n",
        "print(\" Confustion Matrix:\\n \" + str(confusion))\n",
        "print(\"\")\n",
        "print(\" Test Accuracy: {}\".format(accuracy(confusion)))\n",
        "print(\"\")\n",
        "print(\" Test Correct Recall: {}\".format(recall(confusion, CORRECT)))\n",
        "print(\" Test Incorrect Recall: {}\".format(recall(confusion, INCORRECT)))\n",
        "print(\" Test Total Recall: {}\".format(macro_avg_recall(confusion)))\n",
        "print(\"\")\n",
        "print(\" Test Correct Precision: {}\".format(precision(confusion, CORRECT)))\n",
        "print(\" Test Incorrect Precision: {}\".format(precision(confusion, INCORRECT)))\n",
        "print(\" Test Total Precision: {}\".format(marco_avg_precision(confusion)))\n",
        "print(\"\")\n",
        "print(\" Test Correct F1: {}\".format(f_one_measure(confusion, CORRECT)))\n",
        "print(\" Test Incorrect F1: {}\".format(f_one_measure(confusion, INCORRECT)))\n",
        "print(\" Test Total F1: {}\".format(avg_f_one_measure(confusion)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HM5ZgLZCagbB"
      },
      "source": [
        "##################################################\n",
        "#  Matthew's Correlation Coefficient Evaluation  #\n",
        "##################################################\n",
        "\n",
        "# Evaluate Each Test Batch using Matthew's correlation coefficient\n",
        "from sklearn.metrics import matthews_corrcoef \n",
        "\n",
        "matthews_set = []\n",
        "\n",
        "for i in range(len(true_labels)):\n",
        "  matthews = matthews_corrcoef(true_labels[i], \n",
        "                               np.round(sigmoid(predictions[i])))\n",
        "\n",
        "  matthews_set.append(matthews)\n",
        "\n",
        "matthews_corrcoef(flat_true_labels, flat_predictions)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}